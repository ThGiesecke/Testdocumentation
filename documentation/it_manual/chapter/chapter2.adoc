=== Systemvoraussetzungen und Abhängigkeiten

==== Hardwareanforderungen

Für den produktiven Betrieb der Applikation {application} gelten folgende, lastbedingte Mindestanforderungen:
 
===== Entry Level (RAG, 8 GB RAM)

Für kleinere Datenmengen und geringe bis moderate Nutzerzahlen:

* Dokumentensammlung bis ca. 100 MB
* Geringe bis moderate gleichzeitige Nutzer

|===
| *Arbeitsspeicher (vRAM)*	| 8 GB
| *Prozessor (vCPU)*		| 4 vCPUs
| *Speicher*				| 250 GB SSD (virtueller NVMe/SSD-Speicher)
|===

===== Mid-Tier Level (RAG, 32 GB RAM)

Für wachsende Datenmengen und höhere Nutzerzahlen:

* Dokumentensammlung bis ca. 5 GB
* Moderate bis hohe gleichzeitige Nutzerzahlen

|===
| *Arbeitsspeicher (vRAM)*	| 32 GB
| *Prozessor (vCPU)*		| 8-12 vCPUs
| *Speicher*				| 1 TB SSD (virtueller NVMe/SSD-Speicher)
|===

===== High-End Level (RAG, 64 GB RAM)

Für große Datenmengen, hohe Nutzerzahlen oder rechenintensive Aufgaben:

* Dokumentensammlung über 5 GB
* Viele gleichzeitige Nutzer bzw. komplexe, rechenintensive Abfragen

|===
| *Arbeitsspeicher (vRAM)*	| 64 GB
| *Prozessor (vCPU)*		| 16-32 vCPUs
| *Speicher*				| 2 TB SSD (virtueller NVMe/SSD-Speicher)
|===

==== Allgemeine Vorrausetzungen

|===
| *Infrastruktur (VM oder Server)*				| Linux-basierter Server (lokale VM oder Cloud-VM)
| *Betriebssystem*								| Linux, z. B. Ubuntu oder vergleichbare Distributionen
| *Docker-Umgebung*								| Docker Engine: Version ≥ 20.x 
|												| Docker Compose: Version ≥ 1.29 oder Compose V2
| *CPU, RAM, Speicher*							| Abhängig von der Größe der Dokumentensammlung und der Zahl der Benutzer
| *Gemeinsames Verzeichnis* *(SHARED_FOLDER)*	| Ein beschreibbares Verzeichnis auf dem Server für den Datenaustausch
| *Internetzugang / Proxy*						| Download der Docker-Images
|												| Verbindung zur Azure OpenAI API
|												| Proxy-Unterstützung ist möglich
| *Netzwerk / Ports*							| Standardzugang: Port 8000 (bei Bedarf anpassbar)
|===

*Proxy-Unterstützung:* Die Lösung unterstützt den Betrieb hinter einem Proxy. Proxy-Konfiguration kann in den Umgebungsvariablen hinterlegt werden.

*MCP Server API:* Auf Wunsch kundenspezifische Erweiterung der Suchfunktionen für Web-, lokale Kunden-, Bild-, Video- und Nachrichtensuchen sowie KI gestützte Zusammenfassungen 

ifeval::[{big-output} == 1]
include::chapter2-2-o.adoc[]
endif::[]

==== Notwendige Drittanbieter-Komponenten

Folgende Drittanbieter-Komponenten sind für den störungsfreien Betrieb erforderlich und müssen vor der Installation bereitgestellt werden:

**WICHTIG:**{application} stellt **kein LLM** bereit. Der Kunde muss einen der folgenden LLM-Provider selbst einrichten und betreiben.

*Übersicht der unterstützten LLM-Provider*

|===
| Provider | Empfehlung | Vorteile | Setup-Aufwand

| **Azure OpenAI** 
| **Bevorzugt** 
| Enterprise-Support, EU-Regionen, einfache Integration 
| Mittel

| **AWS Bedrock** 
| Gleichwertig  
| Gute Claude-Modelle, AWS-Integration  
| Mittel

| **Lokale Modelle** 
| Für hohe Datenschutzanforderungen  
| Maximaler Datenschutz, keine Cloud  
| Hoch (GPU-Hardware erforderlich) 
|===

**Wählen Sie einen Provider basierend auf:**

- Vorhandener Cloud-Infrastruktur (Azure oder AWS)
- Datenschutz- und Compliance-Anforderungen
- Budget und Betriebsmodell

Der Bezug und die Wartung der Drittanbieter-Komponenten liegen im Verantwortungsbereich der IT-Abteilung.

ifeval::[{big-output} == 1]
include::chapter2-3-o.adoc[]
endif::[]


